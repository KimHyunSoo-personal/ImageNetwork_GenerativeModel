{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "30474a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import yaml\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9adc63fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_download(path):\n",
    "\n",
    "    transform = torchvision.transforms.ToTensor()\n",
    "    dataset = torchvision.datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "\n",
    "    # 저장할 루트 폴더\n",
    "    save_root = path\n",
    "\n",
    "    # 0~9 디렉토리 생성\n",
    "    for digit in range(10):\n",
    "        os.makedirs(os.path.join(save_root, str(digit)), exist_ok=True)\n",
    "\n",
    "    # 이미지 저장\n",
    "    for idx, (img, label) in enumerate(dataset):\n",
    "        save_path = os.path.join(save_root, str(label), f\"{idx}.png\")\n",
    "        torchvision.utils.save_image(img, save_path)\n",
    "\n",
    "    print(\"저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9e2b37ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataset(Dataset):\n",
    "    r\"\"\"\n",
    "    Nothing special here. Just a simple dataset class for mnist images.\n",
    "    Created a dataset class rather using torchvision to allow\n",
    "    replacement with any other image dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, split, im_path, im_ext='png'):\n",
    "        r\"\"\"\n",
    "        Init method for initializing the dataset properties\n",
    "        :param split: train/test to locate the image files\n",
    "        :param im_path: root folder of images\n",
    "        :param im_ext: image extension. assumes all\n",
    "        images would be this type.\n",
    "        \"\"\"\n",
    "        self.split = split\n",
    "        self.im_ext = im_ext\n",
    "        self.images, self.labels = self.load_images(im_path)\n",
    "    \n",
    "    def load_images(self, im_path):\n",
    "        r\"\"\"\n",
    "        Gets all images from the path specified\n",
    "        and stacks them all up\n",
    "        :param im_path:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert os.path.exists(im_path), \"images path {} does not exist\".format(im_path)\n",
    "        ims = []\n",
    "        labels = []\n",
    "        for d_name in tqdm(os.listdir(im_path)):\n",
    "            for fname in glob.glob(os.path.join(im_path, d_name, '*.{}'.format(self.im_ext))):\n",
    "                ims.append(fname)\n",
    "                labels.append(int(d_name))\n",
    "        print('Found {} images for split {}'.format(len(ims), self.split))\n",
    "        return ims, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        im = Image.open(self.images[index])\n",
    "        im_tensor = torchvision.transforms.ToTensor()(im)\n",
    "        \n",
    "        # Convert input to -1 to 1 range.\n",
    "        im_tensor = (2 * im_tensor) - 1\n",
    "        return im_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c47c67ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_embedding(time_steps, temb_dim):\n",
    "    r\"\"\"\n",
    "    Convert time steps tensor into an embedding using the\n",
    "    sinusoidal time embedding formula\n",
    "    :param time_steps: 1D tensor of length batch size\n",
    "    :param temb_dim: Dimension of the embedding\n",
    "    :return: BxD embedding representation of B time steps\n",
    "    \"\"\"\n",
    "    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n",
    "    \n",
    "    # factor = 10000^(2i/d_model)\n",
    "    factor = 10000 ** ((torch.arange(\n",
    "        start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2))\n",
    "    )\n",
    "    \n",
    "    # pos / factor\n",
    "    # timesteps B -> B, 1 -> B, temb_dim\n",
    "    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n",
    "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
    "    return t_emb\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Down conv block with attention.\n",
    "    Sequence of following block\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Downsample using 2x2 average pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
    "                 down_sample=True, num_heads=4, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.down_sample = down_sample\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.t_emb_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels)\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [nn.GroupNorm(8, out_channels)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        self.attentions = nn.ModuleList(\n",
    "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.down_sample_conv = nn.Conv2d(out_channels, out_channels,\n",
    "                                          4, 2, 1) if self.down_sample else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            # Resnet block of Unet\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            \n",
    "            # Attention block of Unet\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "            \n",
    "        out = self.down_sample_conv(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MidBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Mid conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Resnet block with time embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads=4, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers+1)\n",
    "            ]\n",
    "        )\n",
    "        self.t_emb_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels)\n",
    "            )\n",
    "            for _ in range(num_layers + 1)\n",
    "        ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers+1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [nn.GroupNorm(8, out_channels)\n",
    "                for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        self.attentions = nn.ModuleList(\n",
    "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                for _ in range(num_layers)]\n",
    "        )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers+1)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "        \n",
    "        # First resnet block\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first[0](out)\n",
    "        out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
    "        out = self.resnet_conv_second[0](out)\n",
    "        out = out + self.residual_input_conv[0](resnet_input)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            # Attention Block\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "            \n",
    "            # Resnet Block\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i+1](out)\n",
    "            out = out + self.t_emb_layers[i+1](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i+1](out)\n",
    "            out = out + self.residual_input_conv[i+1](resnet_input)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Up conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Upsample\n",
    "    1. Concatenate Down block output\n",
    "    2. Resnet block with time embedding\n",
    "    3. Attention Block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample=True, num_heads=4, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.up_sample = up_sample\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.t_emb_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels)\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [\n",
    "                nn.GroupNorm(8, out_channels)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attentions = nn.ModuleList(\n",
    "            [\n",
    "                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2,\n",
    "                                                 4, 2, 1) \\\n",
    "            if self.up_sample else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, out_down, t_emb):\n",
    "        x = self.up_sample_conv(x)\n",
    "        x = torch.cat([x, out_down], dim=1)\n",
    "        \n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            \n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e67c5af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, model_config):\n",
    "        super().__init__()\n",
    "        im_channels = model_config['im_channels']\n",
    "        self.down_channels = model_config['down_channels']\n",
    "        self.mid_channels = model_config['mid_channels']\n",
    "        self.t_emb_dim = model_config['time_emb_dim']\n",
    "        self.down_sample = model_config['down_sample']\n",
    "        self.num_down_layers = model_config['num_down_layers']\n",
    "        self.num_mid_layers = model_config['num_mid_layers']\n",
    "        self.num_up_layers = model_config['num_up_layers']\n",
    "        \n",
    "        assert self.mid_channels[0] == self.down_channels[-1]\n",
    "        assert self.mid_channels[-1] == self.down_channels[-2]\n",
    "        assert len(self.down_sample) == len(self.down_channels) - 1\n",
    "        \n",
    "        # Initial projection from sinusoidal time embedding\n",
    "        self.t_proj = nn.Sequential(\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n",
    "        )\n",
    "\n",
    "        self.up_sample = list(reversed(self.down_sample))\n",
    "        self.conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size=3, padding=(1, 1))\n",
    "        \n",
    "        self.downs = nn.ModuleList([])\n",
    "        for i in range(len(self.down_channels)-1):\n",
    "            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i+1], self.t_emb_dim,\n",
    "                                        down_sample=self.down_sample[i], num_layers=self.num_down_layers))\n",
    "        \n",
    "        self.mids = nn.ModuleList([])\n",
    "        for i in range(len(self.mid_channels)-1):\n",
    "            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i+1], self.t_emb_dim,\n",
    "                                      num_layers=self.num_mid_layers))\n",
    "        \n",
    "        self.ups = nn.ModuleList([])\n",
    "        for i in reversed(range(len(self.down_channels)-1)):\n",
    "            self.ups.append(UpBlock(self.down_channels[i] * 2, self.down_channels[i-1] if i != 0 else 16,\n",
    "                                    self.t_emb_dim, up_sample=self.down_sample[i], num_layers=self.num_up_layers))\n",
    "        \n",
    "        self.norm_out = nn.GroupNorm(8, 16)\n",
    "        self.conv_out = nn.Conv2d(16, im_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        # Shapes assuming downblocks are [C1, C2, C3, C4]\n",
    "        # Shapes assuming midblocks are [C4, C4, C3]\n",
    "        # Shapes assuming downsamples are [True, True, False]\n",
    "        # B x C x H x W\n",
    "        out = self.conv_in(x)\n",
    "        # B x C1 x H x W\n",
    "        \n",
    "        # t_emb -> B x t_emb_dim\n",
    "        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n",
    "        t_emb = self.t_proj(t_emb)\n",
    "        \n",
    "        down_outs = []\n",
    "        \n",
    "        for idx, down in enumerate(self.downs):\n",
    "            down_outs.append(out)\n",
    "            out = down(out, t_emb)\n",
    "        # down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]\n",
    "        # out B x C4 x H/4 x W/4\n",
    "            \n",
    "        for mid in self.mids:\n",
    "            out = mid(out, t_emb)\n",
    "        # out B x C3 x H/4 x W/4\n",
    "        \n",
    "        for up in self.ups:\n",
    "            down_out = down_outs.pop()\n",
    "            out = up(out, down_out, t_emb)\n",
    "            # out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]\n",
    "        out = self.norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.conv_out(out)\n",
    "        # out B x C x H x W\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8f3109f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNoiseScheduler:\n",
    "    r\"\"\"\n",
    "    Class for the linear noise scheduler that is used in DDPM.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_timesteps, beta_start, beta_end):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        \n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alpha_cum_prod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n",
    "        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alpha_cum_prod)\n",
    "        \n",
    "    def add_noise(self, original, noise, t):\n",
    "        r\"\"\"\n",
    "        Forward method for diffusion\n",
    "        :param original: Image on which noise is to be applied\n",
    "        :param noise: Random Noise Tensor (from normal dist)\n",
    "        :param t: timestep of the forward process of shape -> (B,)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        original_shape = original.shape\n",
    "        batch_size = original_shape[0]\n",
    "        \n",
    "        sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n",
    "        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n",
    "        \n",
    "        # Reshape till (B,) becomes (B,1,1,1) if image is (B,C,H,W)\n",
    "        for _ in range(len(original_shape) - 1):\n",
    "            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n",
    "        for _ in range(len(original_shape) - 1):\n",
    "            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n",
    "        \n",
    "        # Apply and Return Forward process equation\n",
    "        return (sqrt_alpha_cum_prod.to(original.device) * original\n",
    "                + sqrt_one_minus_alpha_cum_prod.to(original.device) * noise)\n",
    "        \n",
    "    def sample_prev_timestep(self, xt, noise_pred, t):\n",
    "        r\"\"\"\n",
    "            Use the noise prediction by model to get\n",
    "            xt-1 using xt and the noise predicted\n",
    "        :param xt: current timestep sample\n",
    "        :param noise_pred: model noise prediction\n",
    "        :param t: current timestep we are at\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x0 = ((xt - (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t] * noise_pred)) /\n",
    "              torch.sqrt(self.alpha_cum_prod.to(xt.device)[t]))\n",
    "        x0 = torch.clamp(x0, -1., 1.)\n",
    "        \n",
    "        mean = xt - ((self.betas.to(xt.device)[t]) * noise_pred) / (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t])\n",
    "        mean = mean / torch.sqrt(self.alphas.to(xt.device)[t])\n",
    "        \n",
    "        if t == 0:\n",
    "            return mean, x0\n",
    "        else:\n",
    "            variance = (1 - self.alpha_cum_prod.to(xt.device)[t - 1]) / (1.0 - self.alpha_cum_prod.to(xt.device)[t])\n",
    "            variance = variance * self.betas.to(xt.device)[t]\n",
    "            sigma = variance ** 0.5\n",
    "            z = torch.randn(xt.shape).to(xt.device)\n",
    "            \n",
    "            # OR\n",
    "            # variance = self.betas[t]\n",
    "            # sigma = variance ** 0.5\n",
    "            # z = torch.randn(xt.shape).to(xt.device)\n",
    "            return mean + sigma * z, x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "09515b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "저장 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 220.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60000 images for split train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:14<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:1 | Loss : 0.2179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [04:03<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:2 | Loss : 0.0478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:38<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:3 | Loss : 0.0296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:41<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:4 | Loss : 0.0233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:39<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:5 | Loss : 0.0202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:38<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:6 | Loss : 0.0177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:37<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:7 | Loss : 0.0167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:37<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:8 | Loss : 0.0158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:37<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:9 | Loss : 0.0150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:37<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:10 | Loss : 0.0146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:37<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:11 | Loss : 0.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:38<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:12 | Loss : 0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:38<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:13 | Loss : 0.0133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:37<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:14 | Loss : 0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:38<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:15 | Loss : 0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:41<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:16 | Loss : 0.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:43<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:17 | Loss : 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:36<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:18 | Loss : 0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:35<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:19 | Loss : 0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:37<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:20 | Loss : 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:38<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:21 | Loss : 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:37<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:22 | Loss : 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:38<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:23 | Loss : 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:39<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:24 | Loss : 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:39<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:25 | Loss : 0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:38<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:26 | Loss : 0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:36<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:27 | Loss : 0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:37<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:28 | Loss : 0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:38<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:29 | Loss : 0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:38<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:30 | Loss : 0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:39<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:31 | Loss : 0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:40<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:32 | Loss : 0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:37<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:33 | Loss : 0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:36<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:34 | Loss : 0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:38<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:35 | Loss : 0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:38<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:36 | Loss : 0.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:38<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:37 | Loss : 0.0106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:40<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:38 | Loss : 0.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:37<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:39 | Loss : 0.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:36<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:40 | Loss : 0.0105\n",
      "Done Training ...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('mps' if torch.mps.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    # Read the config file #\n",
    "    with open(args, 'r') as file:\n",
    "        try:\n",
    "            config = yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    #print(config)\n",
    "    ########################\n",
    "    \n",
    "    diffusion_config = config['diffusion_params']\n",
    "    dataset_config = config['dataset_params']\n",
    "    model_config = config['model_params']\n",
    "    train_config = config['train_params']\n",
    "    \n",
    "    # Create the noise scheduler\n",
    "    scheduler = LinearNoiseScheduler(num_timesteps=diffusion_config['num_timesteps'],\n",
    "                                     beta_start=diffusion_config['beta_start'],\n",
    "                                     beta_end=diffusion_config['beta_end'])\n",
    "    \n",
    "    # Download the dataset\n",
    "    image_download(dataset_config['im_path'])\n",
    "\n",
    "    # Create the dataset\n",
    "    mnist = MnistDataset('train', im_path=dataset_config['im_path'])\n",
    "    mnist_loader = DataLoader(mnist, batch_size=train_config['batch_size'], shuffle=True, num_workers=0)\n",
    "    \n",
    "    # Instantiate the model\n",
    "    model = Unet(model_config).to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # Create output directories\n",
    "    if not os.path.exists(train_config['task_name']):\n",
    "        os.mkdir(train_config['task_name'])\n",
    "    \n",
    "    # Load checkpoint if found\n",
    "    if os.path.exists(os.path.join(train_config['task_name'],train_config['ckpt_name'])):\n",
    "        print('Loading checkpoint as found one')\n",
    "        model.load_state_dict(torch.load(os.path.join(train_config['task_name'],\n",
    "                                                      train_config['ckpt_name']), map_location=device))\n",
    "    # Specify training parameters\n",
    "    num_epochs = train_config['num_epochs']\n",
    "    optimizer = Adam(model.parameters(), lr=train_config['lr'])\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    # Run training\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        losses = []\n",
    "        for im in tqdm(mnist_loader):\n",
    "            optimizer.zero_grad()\n",
    "            im = im.float().to(device)\n",
    "            \n",
    "            # Sample random noise\n",
    "            noise = torch.randn_like(im).to(device)\n",
    "            \n",
    "            # Sample timestep\n",
    "            t = torch.randint(0, diffusion_config['num_timesteps'], (im.shape[0],)).to(device)\n",
    "            \n",
    "            # Add noise to images according to timestep\n",
    "            noisy_im = scheduler.add_noise(im, noise, t)\n",
    "            noise_pred = model(noisy_im, t)\n",
    "\n",
    "            loss = criterion(noise_pred, noise)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('Finished epoch:{} | Loss : {:.4f}'.format(\n",
    "            epoch_idx + 1,\n",
    "            np.mean(losses),\n",
    "        ))\n",
    "        torch.save(model.state_dict(), os.path.join(train_config['task_name'],\n",
    "                                                    train_config['ckpt_name']))\n",
    "    \n",
    "    print('Done Training ...')\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train('default.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "73fba6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_params': {'im_path': 'data/train/images'}, 'diffusion_params': {'num_timesteps': 1000, 'beta_start': 0.0001, 'beta_end': 0.02}, 'model_params': {'im_channels': 3, 'im_size': 28, 'down_channels': [32, 64, 128, 256], 'mid_channels': [256, 256, 128], 'down_sample': [True, True, False], 'time_emb_dim': 128, 'num_down_layers': 2, 'num_mid_layers': 2, 'num_up_layers': 2, 'num_heads': 4}, 'train_params': {'task_name': 'default', 'batch_size': 64, 'num_epochs': 40, 'num_samples': 100, 'num_grid_rows': 10, 'lr': 0.0001, 'ckpt_name': 'ddpm_ckpt.pth'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [01:56,  8.58it/s]\n"
     ]
    }
   ],
   "source": [
    "def sample(model, scheduler, train_config, model_config, diffusion_config):\n",
    "    r\"\"\"\n",
    "    Sample stepwise by going backward one timestep at a time.\n",
    "    We save the x0 predictions\n",
    "    \"\"\"\n",
    "    xt = torch.randn((train_config['num_samples'],\n",
    "                      model_config['im_channels'],\n",
    "                      model_config['im_size'],\n",
    "                      model_config['im_size'])).to(device)\n",
    "    for i in tqdm(reversed(range(diffusion_config['num_timesteps']))):\n",
    "        # Get prediction of noise\n",
    "        noise_pred = model(xt, torch.as_tensor(i).unsqueeze(0).to(device))\n",
    "        \n",
    "        # Use scheduler to get x0 and xt-1\n",
    "        xt, x0_pred = scheduler.sample_prev_timestep(xt, noise_pred, torch.as_tensor(i).to(device))\n",
    "        \n",
    "        # Save x0\n",
    "        ims = torch.clamp(xt, -1., 1.).detach().cpu()\n",
    "        ims = (ims + 1) / 2\n",
    "        grid = torchvision.utils.make_grid(ims, nrow=train_config['num_grid_rows'])\n",
    "        img = torchvision.transforms.ToPILImage()(grid)\n",
    "        if not os.path.exists(os.path.join(train_config['task_name'], 'samples')):\n",
    "            os.mkdir(os.path.join(train_config['task_name'], 'samples'))\n",
    "        img.save(os.path.join(train_config['task_name'], 'samples', 'x0_{}.png'.format(i)))\n",
    "        img.close()\n",
    "\n",
    "\n",
    "def infer(args):\n",
    "    # Read the config file #\n",
    "    with open(args, 'r') as file:\n",
    "        try:\n",
    "            config = yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    print(config)\n",
    "    ########################\n",
    "    \n",
    "    diffusion_config = config['diffusion_params']\n",
    "    model_config = config['model_params']\n",
    "    train_config = config['train_params']\n",
    "    \n",
    "    # Load model with checkpoint\n",
    "    model = Unet(model_config).to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(train_config['task_name'],\n",
    "                                                  train_config['ckpt_name']), map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # Create the noise scheduler\n",
    "    scheduler = LinearNoiseScheduler(num_timesteps=diffusion_config['num_timesteps'],\n",
    "                                     beta_start=diffusion_config['beta_start'],\n",
    "                                     beta_end=diffusion_config['beta_end'])\n",
    "    with torch.no_grad():\n",
    "        sample(model, scheduler, train_config, model_config, diffusion_config)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    infer('default.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
